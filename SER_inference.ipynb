{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SER_inference.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPPpJ0QcYyzwCVi74jn07vK",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ValeryNikiforov/Speech-Emotion-Recognition/blob/master/SER_inference.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V1aqj8pQNVlj",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 272
        },
        "outputId": "f7f581da-aa0d-4660-9ae7-c40c954bad5a"
      },
      "source": [
        "\n",
        "import numpy as np\n",
        "import scipy.io.wavfile as wavfile\n",
        "from scipy.fftpack import dct\n",
        "!pip install librosa==0.7.2\n",
        "import librosa\n",
        "import torch\n",
        "import torch.nn as nn\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: librosa==0.7.2 in /usr/local/lib/python3.6/dist-packages (0.7.2)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.6/dist-packages (from librosa==0.7.2) (1.18.5)\n",
            "Requirement already satisfied: audioread>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from librosa==0.7.2) (2.1.8)\n",
            "Requirement already satisfied: joblib>=0.12 in /usr/local/lib/python3.6/dist-packages (from librosa==0.7.2) (0.15.1)\n",
            "Requirement already satisfied: six>=1.3 in /usr/local/lib/python3.6/dist-packages (from librosa==0.7.2) (1.12.0)\n",
            "Requirement already satisfied: soundfile>=0.9.0 in /usr/local/lib/python3.6/dist-packages (from librosa==0.7.2) (0.10.3.post1)\n",
            "Requirement already satisfied: resampy>=0.2.2 in /usr/local/lib/python3.6/dist-packages (from librosa==0.7.2) (0.2.2)\n",
            "Requirement already satisfied: scikit-learn!=0.19.0,>=0.14.0 in /usr/local/lib/python3.6/dist-packages (from librosa==0.7.2) (0.22.2.post1)\n",
            "Requirement already satisfied: numba>=0.43.0 in /usr/local/lib/python3.6/dist-packages (from librosa==0.7.2) (0.48.0)\n",
            "Requirement already satisfied: scipy>=1.0.0 in /usr/local/lib/python3.6/dist-packages (from librosa==0.7.2) (1.4.1)\n",
            "Requirement already satisfied: decorator>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from librosa==0.7.2) (4.4.2)\n",
            "Requirement already satisfied: cffi>=1.0 in /usr/local/lib/python3.6/dist-packages (from soundfile>=0.9.0->librosa==0.7.2) (1.14.0)\n",
            "Requirement already satisfied: llvmlite<0.32.0,>=0.31.0dev0 in /usr/local/lib/python3.6/dist-packages (from numba>=0.43.0->librosa==0.7.2) (0.31.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from numba>=0.43.0->librosa==0.7.2) (47.1.1)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.6/dist-packages (from cffi>=1.0->soundfile>=0.9.0->librosa==0.7.2) (2.20)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VabEqXNCNjkQ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "ce968c58-b254-42ef-c7c4-d86e1218aa4d"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive',force_remount=True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JxLaxABWNu-A",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "emotions = {0:'angry', 1:'disgust', 2:'fear', 3:'happy', 4:'neutral', 5:'surprise', 6:'sad'}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BCKBI7c22Ocd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def mfcc(X_raw_data):\n",
        "  X_raw_data = np.expand_dims(X_raw_data,axis=0)\n",
        "  first_fragment_size = 297\n",
        "  sec_fragment_size = 200\n",
        "  frame_size = 0.025\n",
        "  frame_stride = 0.01\n",
        "  sample_rate = 8000\n",
        "  num = np.shape(X_raw_data)[0]\n",
        "  frame_length, frame_step = frame_size * sample_rate, frame_stride * sample_rate  \n",
        "  X_frames = np.zeros((num,first_fragment_size,sec_fragment_size))\n",
        "  for i in range(len(X_raw_data)):\n",
        "    signal_length = len(X_raw_data[i])\n",
        "    frame_length = int(round(frame_length))\n",
        "    frame_step = int(round(frame_step))\n",
        " \n",
        "    num_frames = 1+int(np.ceil(float(signal_length - frame_length) / frame_step)) \n",
        "    pad_signal_length = int((num_frames-1) * frame_step + frame_length)\n",
        "    z = np.zeros((pad_signal_length - signal_length))\n",
        "    pad_signal = np.append(X_raw_data[i], z) \n",
        "    for j in range(len( X_frames[i])):\n",
        "      X_frames[i,j] *= np.hamming(frame_length)\n",
        "  \n",
        "    indices = np.tile(np.arange(0, frame_length), (num_frames, 1)) + np.tile(np.arange(0, num_frames * frame_step, frame_step), (frame_length, 1)).T\n",
        "    X_frames[i] = pad_signal[indices.astype(np.int32, copy=False)]\n",
        "  del z, pad_signal, X_raw_data\n",
        "  NFFT = 512\n",
        "  pow_frames = np.zeros((num,first_fragment_size,257))\n",
        "  for i in range(len(X_frames)):\n",
        "    pow_frames[i] = ((1.0 / NFFT) * (np.square(np.absolute(np.fft.rfft(X_frames[i], NFFT)))))\n",
        "  del X_frames\n",
        "\n",
        "  nfilt = 26\n",
        "  low_freq_mel = 0\n",
        "  high_freq_mel = (2595 * np.log10(1 + (sample_rate / 2) / 700.)) \n",
        "  mel_points = np.linspace(low_freq_mel, high_freq_mel, nfilt + 2)  \n",
        "  hz_points = (700 * (10**(mel_points / 2595.0) - 1)) \n",
        "  bin = np.floor((NFFT + 1) * hz_points / sample_rate)\n",
        "\n",
        "  fbanks = np.zeros([nfilt,NFFT // 2 + 1])\n",
        "  for j in range(0, nfilt):\n",
        "  \n",
        "    for i in range(int(bin[j]), int(bin[j+1])):\n",
        "            fbanks[j,i] = (i - bin[j]) / (bin[j+1]-bin[j])\n",
        "    for i in range(int(bin[j+1]), int(bin[j+2])):\n",
        "            fbanks[j,i] = (bin[j+2]-i) / (bin[j+2]-bin[j+1])       \n",
        "  filter_banks = np.zeros((num,first_fragment_size,26))   \n",
        "  energy = np.zeros((num,first_fragment_size))\n",
        "  for i in range(num):\n",
        "    filter_banks[i]= np.dot(pow_frames[i], fbanks.T)\n",
        "    filter_banks[i] = np.where(filter_banks[i] == 0, np.finfo(float).eps, filter_banks[i])  \n",
        "    energy[i] = np.sum(pow_frames[i],1) \n",
        "    energy[i] = np.where(energy[i] == 0,np.finfo(float).eps,energy[i])\n",
        "  del pow_frames\n",
        "  num_ceps = 26\n",
        "  mfcc_old = np.zeros((np.shape(filter_banks)[0], first_fragment_size, 26))\n",
        "  _, nframes,ncoeff = np.shape(mfcc_old)\n",
        "  n = np.arange(ncoeff)\n",
        "  lift = 1 + (22/2.)*np.sin(np.pi*n/22)\n",
        "  for i in range(np.shape(filter_banks)[0]):\n",
        "    filter_banks[i]= np.log(filter_banks[i])\n",
        "    mfcc_old[i] = dct(filter_banks[i], type=2, axis=1, norm='ortho')[:, : num_ceps]\n",
        "    mfcc_old[i] = lift * mfcc_old[i]\n",
        "    mfcc_old[i,:,0] = np.log(energy[i])\n",
        "  return  mfcc_old[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vks681NpN4TH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def melspec(X):\n",
        "  X = np.array(X[1], dtype=float)\n",
        "  X = librosa.feature.melspectrogram(X, win_length=1024)\n",
        "  X = np.where(X == 0, np.finfo(float).eps, X)\n",
        "  X = 20*np.log10(X)\n",
        "  return X"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V_Jx65wZOAmb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class NeuralNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(NeuralNet, self).__init__()\n",
        "        \n",
        "        self.input_spec_size=128\n",
        "        self.cnn_filter_size=128\n",
        "        self.num_layers_lstm=1\n",
        "        self.num_heads_self_attn=4\n",
        "        self.hidden_size_lstm=60\n",
        "        self.num_emo_classes=7\n",
        "        \n",
        "        self.conv_1 = nn.Conv1d(self.input_spec_size,self.cnn_filter_size,3,1)\n",
        "        self.max_pooling_1 = nn.MaxPool1d(5)\n",
        "        self.bn = nn.BatchNorm1d(self.cnn_filter_size)\n",
        "        self.conv_2 = nn.Conv1d(self.cnn_filter_size,self.cnn_filter_size,3,1)\n",
        "        self.max_pooling_2 = nn.MaxPool1d(3)\n",
        "        \n",
        "        ###\n",
        "        self.lstm = nn.LSTM(input_size=self.cnn_filter_size, hidden_size=self.hidden_size_lstm,num_layers=self.num_layers_lstm,bidirectional=True,dropout=0.3,batch_first=True)\n",
        "        ## Transformer\n",
        "        self.encoder_layer = nn.TransformerEncoderLayer(d_model=self.hidden_size_lstm*2,dim_feedforward=512, nhead=self.num_heads_self_attn)\n",
        "        self.emotion_layer = nn.Linear(self.hidden_size_lstm*4,self.num_emo_classes)\n",
        "\n",
        "\n",
        "    def forward(self,inputs):\n",
        "        out = self.conv_1(inputs)\n",
        "        out = self.max_pooling_1(out)\n",
        "        out = self.bn(out)\n",
        "        out = self.conv_2(out)\n",
        "        #out = self.max_pooling_2(out)\n",
        "        out = self.bn(out)\n",
        "        out = out.permute(0, 2, 1)\n",
        "        out, (final_hidden_state, final_cell_state) = self.lstm(out)\n",
        "        out = self.encoder_layer(out)\n",
        "        mean = torch.mean(out,1)\n",
        "        std = torch.std(out,1)\n",
        "        stat = torch.cat((mean,std),1)\n",
        "        pred_emo = self.emotion_layer(stat)\n",
        "        return pred_emo"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "edcFPTo0Oi6y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def open_prepare(path, feats_type):\n",
        "  new_record = wavfile.read(path)[1]\n",
        "  if (np.size(new_record) < 23879):\n",
        "    time_differece = 23879-np.size(new_record)\n",
        "    res = np.append(new_record,np.zeros((time_differece)))\n",
        "  else:\n",
        "    res = new_record[:23879] \n",
        "  if feats_type == 'mfcc':\n",
        "    return mfcc(res)\n",
        "  return melspec(res)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OcqWCmz2P5Wd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def predict(features, feats_type):\n",
        "  features = torch.Tensor(features)\n",
        "  features = features.unsqueeze(-1)\n",
        "  if feats_type == 'mfcc':\n",
        "    features = features.permute(2,1,0)\n",
        "    model = torch.load('drive/My Drive/model_mfcc13.pth')\n",
        "  if feats_type == 'melspec':\n",
        "    features = features.permute(2,0,1)\n",
        "    model = torch.load('drive/My Drive/model_spec.pth')\n",
        "  out = model(features)\n",
        "  out = torch.argmax(out)\n",
        "  print(emotions[out.item()])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G_arXNuBS5Zv",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "751f31f8-08e1-4887-ad23-47446d51e457"
      },
      "source": [
        "feats_type = 'mfcc' #'mfcc' or 'melspec'\n",
        "inp=open_prepare('drive/My Drive/YAF_wire_sad.wav', feats_type)\n",
        "predict(inp, feats_type)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(1, 23879)\n",
            "sad\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}